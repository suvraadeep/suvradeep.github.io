---
title: 'Dissecting the Attention Mechanism and Transformers'
date: 2025-03-05
permalink: /posts/2025/03/blog-post-2/
tags:
  - Transformers
  - LLM
  - Deepseek
  - Seq2seq
---

It is my second blog where I’ll share everything I know about Transformer and its heart Attention mechanism.So we will take a deep dive or rather dissect the Transformer architecture, with a special focus on various attention mechanisms we’ll explore their evolution from foundational approaches like SEQ2SEQ to Bahdanau attention to advanced techniques such as Multihead and Cross attention

Headings are cool
======

You can have many headings
======

Aren't headings cool?
------
