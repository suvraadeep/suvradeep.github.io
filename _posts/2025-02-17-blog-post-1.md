---
title: 'Tokenization Demystified: Building Tokenizers for Language Models'
date: 2025-02-17
permalink: /posts/2025/02/blog-post-1/
tags:
  - LLM
  - Tokenization
  - Naturallanguageprocessing
  - Datascience
--- 

Cool! So this is my very first blog where I’ll share everything I know about tokenizers. I’ve reviewed other blogs on the topic, but none seemed as comprehensive as the one I’m about to present.  

By the end of this, you’ll have a solid understanding of the first step in the LLM pre-training lifecycle — **tokenization**. Without this crucial step, developing LLMs would be incredibly challenging :)  

And I must admit, I’m writing for a selfish purpose — it’s for my own learning. As Feynman said, *when you teach, you learn better.*  

For reference, I have used:  
- [Hands-On Large Language Models: Language Understanding and Generation](https://www.amazon.in/Hands-Large-Language-Models-Understanding/dp/1098150961)  
- [Building LLM from Scratch Playlist](https://youtube.com/playlist?list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&si=R4L7305IMJRb73vl)  


## Introduction
Tokens and embeddings are fundamental concepts in large language models (LLMs) so understanding these concepts is crucial not only for grasping the history of Language AI but also for comprehending how LLMs function, how they are built, and their future potential. Since LLMs cannot process text like humans, we must convert text into numerical representations that capture semantic meaning and emotions.

Most LLMs operate as “next-word predictors” in their pre-trained form and are later fine-tuned for tasks like chat assistant (Like ChatGPT) or text classification. You may notice that a model does not generate its response all at once but rather one token at a time. Tokens are not just the model’s output; they also serve as its input representation. as during model design, the team first selects a tokenization method. Common approaches include Byte Pair Encoding (BPE), widely used in GPT models, and WordPiece, used in BERT. Both methods aim to create an efficient set of tokens for representing text but achieve this in different ways. After selecting a method, several design choices must be made, such as vocabulary size and the inclusion of special tokens after that the tokenizer is then trained on a specific dataset to learn the optimal vocabulary for that data. Even with the same methods and parameters, a tokenizer trained on English text will differ from one trained on code or multilingual datasets.

So in short when a text prompt is given to an LLM, it is initially divided into tokens — a process managed by tokenizers. Different language models use different tokenizers; for example, the image below illustrates how GPT-4’s tokenizer breaks text into smaller segments. You can also find an example showcasing the tokenizer in action.





Aren't headings cool?
------
